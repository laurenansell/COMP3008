---
title: "COMP3008 - Big Data Analytics"
author: "Workshop 8 - Document Classification"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this worksheet we will be using R to perform document classification and produce different visualisations illustrating each of the different techniques. All the data required for this practical is available on the DLE. 

The data is contained in the three files \texttt{text\_1.txt}, \texttt{text\_2.txt} and \texttt{text\_3.txt}. Each file contains the text from an entire book. Each book is from a different genre, the genres are horror, fantasy and romance. Your tasks are to apply each of the methods below and classify the texts to the correct genre. 

```{r,warning=FALSE,message=FALSE}
## Libraries required 

library(tidyverse)
library(tidytext)
library(wordcloud)
library(wordcloud2)
library(ggpubr)
library(igraph)
library(ggraph)

```

## Exercise 1 - Bag-of-words

The first exercise is to apply the basic technique, bag-of-words, to each of the texts. As we saw in the lecture, there will be common words in all the texts which will occupy the "top spots" when we count the frequencies. Included the in the \texttt{tidytext} package is a dictionary of stop words (common words), we can use the \texttt{anti\_join()} function to remove these from our text.

Visualise the results as a word cloud and a bar chart showing the top 15 most common words.

\textbf{Solution}

```{r,message=FALSE,warning=FALSE}

## Read in the data
book_1<-read.delim("../Lab Sheets/text_1.txt",header = FALSE)
book_2<-read.delim("../Lab Sheets/text_2.txt",header = FALSE)
book_3<-read.delim("../Lab Sheets/text_3.txt",header = FALSE)


## Tokenise the data

book_1_words<-book_1 %>% select(V1) %>%  unnest_tokens(word, V1)
book_2_words<-book_2 %>% select(V1) %>%  unnest_tokens(word, V1)
book_3_words<-book_3 %>% select(V1) %>%  unnest_tokens(word, V1)

## Remove the stop words

book_1_words_clean<-book_1_words %>% anti_join(stop_words)
book_2_words_clean<-book_2_words %>% anti_join(stop_words)
book_3_words_clean<-book_3_words %>% anti_join(stop_words)
```

Visualise as a bar plot:

```{r}
## Plotting the top 15 words in a bar chart

book_1_words_clean %>% group_by(word) %>% count(sort = TRUE) %>% head(15) %>% 
  ggplot( aes(x = word, y = n)) +
  geom_col(fill = "cyan", color = "blue") +
  coord_flip() +
  labs(x = "Unique words",
       y = "Frequency",
       title = "Count of unique words found") + 
  theme(axis.text = element_text(size = 12, color = "black"), 
        axis.title = element_text(size = 12, color = "black"),
        title = element_text(size = 14))

book_2_words_clean %>% group_by(word) %>% count(sort=TRUE) %>% head(15) %>% 
  ggplot( aes(x = word, y = n)) +
  geom_col(fill = "cyan", color = "blue") +
  coord_flip() +
  labs(x = "Unique words",
       y = "Frequency",
       title = "Count of unique words found") + 
  theme(axis.text = element_text(size = 12, color = "black"), 
        axis.title = element_text(size = 12, color = "black"),
        title = element_text(size = 14))

book_3_words_clean %>% group_by(word) %>% count(sort = TRUE) %>% head(15) %>% 
  ggplot( aes(x = word, y = n)) +
  geom_col(fill = "cyan", color = "blue") +
  coord_flip() +
  labs(x = "Unique words",
       y = "Frequency",
       title = "Count of unique words found") + 
  theme(axis.text = element_text(size = 12, color = "black"), 
        axis.title = element_text(size = 12, color = "black"),
        title = element_text(size = 14))
```

\textcolor{blue}{We see in text 1 that words such as wizard, magic and dragon are common, therefore we could classify this text as fantasy. In text 2 we see names of characters appearing frequently and the word love is a common word, this could lead us to classifying this text as romance. The final text, text 3 has Carrie as the most common word, this is probably the main charaters name and we see the word blood has a high frequency which would lead us to classifing this text as horror.}

\textcolor{blue}{In some cases we may wish to remove additional words, here we may want to remove names and other words not included in the stop words such as didn't. The way to do this is shown below}


```{r}
## Create a dataframe containing our stopwords
my_stop_words<-data.frame(word=c("wasn’t","didn’t","he’d","don’t","she’d","couldn’t",
                                 "it’s","looked"))

## remove these from our books
book_1_words_clean<-book_1_words_clean %>% anti_join(my_stop_words)
book_2_words_clean<-book_2_words_clean %>% anti_join(my_stop_words)
book_3_words_clean<-book_3_words_clean %>% anti_join(my_stop_words)

## Now plot the data
book_1_words_clean %>% group_by(word) %>% count(sort = TRUE) %>% head(15) %>% 
  ggplot( aes(x = word, y = n)) +
  geom_col(fill = "cyan", color = "blue") +
  coord_flip() +
  labs(x = "Unique words",
       y = "Frequency",
       title = "Count of unique words found") + 
  theme(axis.text = element_text(size = 12, color = "black"), 
        axis.title = element_text(size = 12, color = "black"),
        title = element_text(size = 14))

book_2_words_clean %>% group_by(word) %>% count(sort=TRUE) %>% head(15) %>% 
  ggplot( aes(x = word, y = n)) +
  geom_col(fill = "cyan", color = "blue") +
  coord_flip() +
  labs(x = "Unique words",
       y = "Frequency",
       title = "Count of unique words found") + 
  theme(axis.text = element_text(size = 12, color = "black"), 
        axis.title = element_text(size = 12, color = "black"),
        title = element_text(size = 14))

book_3_words_clean %>% group_by(word) %>% count(sort = TRUE) %>% head(15) %>% 
  ggplot( aes(x = word, y = n)) +
  geom_col(fill = "cyan", color = "blue") +
  coord_flip() +
  labs(x = "Unique words",
       y = "Frequency",
       title = "Count of unique words found") + 
  theme(axis.text = element_text(size = 12, color = "black"), 
        axis.title = element_text(size = 12, color = "black"),
        title = element_text(size = 14))
```



```{r,eval=FALSE}
## Word cloud for each text

book_1_text_wordcloud<- book_1_words_clean %>%
  count(word, sort = TRUE) %>% 
  mutate(freq = n / sum(n))

book_2_text_wordcloud<- book_2_words_clean %>%
  count(word, sort = TRUE) %>% 
  mutate(freq = n / sum(n))

book_3_text_wordcloud<- book_3_words_clean %>%
  count(word, sort = TRUE) %>% 
  mutate(freq = n / sum(n))

book_1_text_wordcloud<-book_1_text_wordcloud %>% select(word,freq)
book_2_text_wordcloud<-book_2_text_wordcloud %>% select(word,freq)
book_3_text_wordcloud<-book_3_text_wordcloud %>% select(word,freq)

wordcloud2(book_1_text_wordcloud,color = "random-dark",shape="star")
wordcloud2(book_2_text_wordcloud,color = "random-dark",shape="cardioid")
wordcloud2(book_3_text_wordcloud,figPath = "skull.jpg")

```

![](text_1_wordcloud.png)

![](text_2_wordcloud.png)

![](text_3_wordcloud.png)
\textcolor{blue}{In all the word coulds we see character names appearing the most frequently. We see in text 1 that words such as wizard, magic and dragon are common, therefore we could classify this text as fantasy. In text 2 we see names of characters appearing frequently and the word love is a common word, this could lead us to classifying this text as romance. The final text, text 3 has Carrie as the most common word, this is probably the main charaters name and we see the word blood has a high frequency which would lead us to classifing this text as horror.}

\textcolor{red}{\textbf{Making these wordclouds}: These wordclouds are made using the package wordcloud2. They will not appear in the usual Plots pane but in the Viewer one instead. They are also dynamic, they don't add all the words in one go but in a sequence. If you want to use a different shape from a .jpg file, there is a bug with this. It will not appear in the Viewer pane, you will need to view this in your browser by clicking the window with an arrow icon immediately above the white area where the plot should appear.}


## Exercise 2 - N-grams

The second exercise is to extend the bag-of-words method to n-grams. The number of words to be included as up to you, there may be an element of trial and error to find the n-gram which is the most useful. 

As with the first exercise, you will need to ensure that stop words have been removed. 


Visualise the results as a word cloud and a network plot showing the connections within the text.

\textbf{Solution}

\textcolor{blue}{Below is the code for 2,3 and 4 -grams on the first book only. For the other two books the input needs to be altered but the stucture and logic of the code remains the same}

```{r,eval=FALSE}
## bigrams
book_1_2_grams<- book_1 %>%
  dplyr::select(V1) %>%
  unnest_tokens(paired_words, V1, token = "ngrams", n = 2)


book_1_2_grams_seperated <- book_1_2_grams %>%
  separate(paired_words, c("word1", "word2"), sep = " ")

book_1_2_grams_seperated <- book_1_2_grams_seperated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

book_1_bigrams <- book_1_2_grams_seperated %>% 
  count(word1, word2, sort = TRUE)

book_1_bigrams<-book_1_bigrams %>% na.omit()

book_1_bigrams_graph <- book_1_bigrams %>%
  filter(n > 2) %>%
  graph_from_data_frame()
ggraph(book_1_bigrams_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1,size=4)+
  theme_graph(base_size=10)+
  ggtitle("Network of words")

## 3-grams

book_1_3_grams<- book_1 %>%
  dplyr::select(V1) %>%
  unnest_tokens(trip_words, V1, token = "ngrams", n = 3)


book_1_3_grams_seperated <- book_1_3_grams %>%
  separate(trip_words, c("word1", "word2","word3"), sep = " ")

book_1_3_grams_seperated <- book_1_3_grams_seperated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)

book_1_tripgrams <- book_1_3_grams_seperated %>% 
  count(word1, word2,word3, sort = TRUE)

book_1_tripgrams<-book_1_tripgrams %>% na.omit()

book_1_tripgrams_graph <- book_1_tripgrams %>%
  filter(n > 1) %>%
  graph_from_data_frame()
ggraph(book_1_tripgrams_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1,size=4)+
  theme_graph(base_size=10)+
  ggtitle("Network of words")

## 4-grams
book_1_4_grams<- book_1 %>%
  dplyr::select(V1) %>%
  unnest_tokens(quad_words, V1, token = "ngrams", n = 4)


book_1_4_grams_seperated <- book_1_4_grams %>%
  separate(quad_words, c("word1", "word2","word3","word4"), sep = " ")

book_1_4_grams_seperated <- book_1_4_grams_seperated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word) %>%
  filter(!word4 %in% stop_words$word)

book_1_quadgrams <- book_1_4_grams_seperated %>% 
  count(word1, word2,word3,word4, sort = TRUE)

book_1_quadgrams<-book_1_quadgrams %>% na.omit()

book_1_quadgrams_graph <- book_1_quadgrams %>%
  filter(n > 0) %>%
  graph_from_data_frame()
ggraph(book_1_quadgrams_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1,size=4)+
  theme_graph(base_size=10)+
  ggtitle("Network of words")
```

![](2-gram.png)

![](3-grams.png)

![](4-grams.png)


```{r}
## Wordclouds

book_1_2_grams<- book_1 %>%
  dplyr::select(V1) %>%
  unnest_tokens(paired_words, V1, token = "ngrams", n = 2)

wordcloud(book_1_2_grams
        , scale=c(5,0.5)     # Set min and max scale
        , max.words=50      # Set top n words
        , random.order=FALSE # Words in decreasing freq
        , rot.per=0.35       # % of vertical words
        , use.r.layout=FALSE # Use C++ collision detection
        , colors=brewer.pal(8, "Dark2"))

book_1_3_grams<- book_1 %>%
  dplyr::select(V1) %>%
  unnest_tokens(trip_words, V1, token = "ngrams", n = 3)

wordcloud(book_1_3_grams
        , scale=c(5,0.5)     # Set min and max scale
        , max.words=50      # Set top n words
        , random.order=FALSE # Words in decreasing freq
        , rot.per=0.35       # % of vertical words
        , use.r.layout=FALSE # Use C++ collision detection
        , colors=brewer.pal(8, "Dark2"))

book_1_4_grams<- book_1 %>%
  dplyr::select(V1) %>%
  unnest_tokens(quad_words, V1, token = "ngrams", n = 4)

wordcloud(book_1_4_grams
        , scale=c(5,0.5)     # Set min and max scale
        , max.words=50      # Set top n words
        , random.order=FALSE # Words in decreasing freq
        , rot.per=0.35       # % of vertical words
        , use.r.layout=FALSE # Use C++ collision detection
        , colors=brewer.pal(8, "Dark2"))

```

## Exercise 3 - tf-idf

For the third exercise we will be calculating the tf-idf values for each text using the \texttt{bind\_tf\_idf()} function. Once again you will need to remove the stop words from the text and plot your results on a bar plot.

\textbf{Solution}

```{r}

## If we combine all the data from the three books we can then plot them on the same graph using 
## facet_wrap

book_1_words_tf_idf<-book_1_words_clean %>% mutate(book="book 1")
book_2_words_tf_idf<-book_2_words_clean %>% mutate(book="book 2")
book_3_words_tf_idf<-book_3_words_clean %>% mutate(book="book 3")

all_books_tf_idf<-rbind(book_1_words_tf_idf,book_2_words_tf_idf,book_3_words_tf_idf)

tf_idf_df <- all_books_tf_idf %>%
  count(book, word) %>%
  bind_tf_idf(word, book, n) %>%
  arrange(desc(tf_idf)) 

tf_idf_df %>%
  group_by(book) %>%
  top_n(10, tf_idf) %>% ggplot(aes(x=word, y=tf_idf,fill=book))+
  geom_bar(stat = "identity")+facet_wrap(.~book, scales = "free")+
  coord_flip()+theme(legend.position = "none")

```
\textcolor{blue}{Once again we see character names being highlighted by the method. Magic is still being highlighted in book 1, reinforcing the classification as fantasy.}


## Exercise 4 - Zipf's Law

In the final exercise, you will be applying Zipf's Law to each of the texts. For this we need the term frequency which is calculated in the function used for exercise 3. You will then need to create a column which contains the rank of each word, grouped by text, in descending order, i.e. the word with the highest tf value has rank 1.

Show your results on a line plot, plotting the log of the rank against the log of the term frequency, and add the linear equation for each line to the plot.

\textbf{Solution}

```{r}

## Put all the books together
book_1_words<-book_1_words %>% mutate(book="book 1")
book_2_words<-book_2_words %>% mutate(book="book 2")
book_3_words<-book_3_words %>% mutate(book="book 3")

all_books<-rbind(book_1_words,book_2_words,book_3_words)


zipfs_law<-all_books %>%
  count(book,word) %>%
  bind_tf_idf(word,book,n)

zipfs_law<-zipfs_law %>%  arrange(desc(tf)) %>% group_by(book) %>% 
  mutate(rank=row_number())

## Plot all the data together

ggplot(zipfs_law,aes(x=log(rank),y=log(tf),col=book))+geom_line(linewidth=1)+
  geom_smooth(method = "lm",se=FALSE, linetype=2, linewidth=0.5)+
  stat_regline_equation(label.x =4.5)

```


\textcolor{blue}{Books 1 and 3 have very similar equations, book 2 stands out as being very different. When making the classifications, this result would lead us to either classifing books 1 and 3 to the same genre or to genres which could be very closely linked.}
